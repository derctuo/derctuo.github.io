<!DOCTYPE html>
<html><title>Methods for two-dimensional rotation with two or three real multiplies ⁑ Derctuo</title><meta charset="utf-8"></meta><link href="../liabilities/style.css" rel="stylesheet"></link><meta content="width=device-width, initial-scale=1.0" name="viewport"></meta><h1>Methods for two-dimensional rotation with two or three real multiplies</h1><div class="metadata">Kragen Javier Sitaker, 02020-12-23 (updated 02020-12-26)
(14 minutes)</div>
<h2>Method K for complex multiplication</h2>
<p>Wikipedia says Knuth gives an algorithm for multiplying (<em>a</em> +
<em>bi</em>)(<em>c</em> + <em>di</em>) as follows: (<em>k</em>₁ - <em>k</em>₂) + (<em>k</em>₁ + <em>k</em>₃)<em>i</em> where
<em>k</em>₁ = <em>a</em>(<em>c</em> + <em>d</em>), <em>k</em>₂ = (<em>a</em> + <em>b</em>)<em>d</em>, <em>k</em>₃ = (<em>b</em> - <em>a</em>)<em>c</em>,
which works out to (<em>ac</em> - <em>bd</em>) + (<em>ad</em> + <em>bc</em>)<em>i</em> as it should.
Call this Method K.  Method K is interesting because often
multiplication is much more expensive than addition or subtraction
(for example, it takes much more space in hardware in fixed point, and
much more time in multiple precision), and this algorithm requires
only three real multiplies, rather than the four required by the more
direct approach.</p>
<p>In Unicode matrix form:</p>
<pre><code>┎             ┒┎  ┒     ┎  ┒
┃  1  1  0  0 ┃┃ac┃     ┃k₁┃
┃  0  1  0  1 ┃┃ad┃  =  ┃k₂┃
┃ -1  0  1  0 ┃┃bc┃     ┃k₃┃
┖             ┚┃bd┃     ┖  ┚
               ┖  ┚

┎           ┒┎  ┒     ┎ ┒
┃  1  -1  0 ┃┃k₁┃     ┃ℜ┃
┃  1   0  1 ┃┃k₂┃  =  ┃ℑ┃
┖           ┚┃k₃┃     ┖ ┚
             ┖  ┚
</code></pre>
<p>Because</p>
<pre><code>┎           ┒┎             ┒┎  ┒
┃  1  -1  0 ┃┃  1  1  0  0 ┃┃ac┃
┃  1   0  1 ┃┃  0  1  0  1 ┃┃ad┃ =
┖           ┚┃ -1  0  1  0 ┃┃bc┃
             ┖             ┚┃bd┃
                            ┖  ┚
               ┎  ┒
┎             ┒┃ac┃
┃  1  0  0 -1 ┃┃ad┃
┃  0  1  1  0 ┃┃bc┃
┖             ┚┃bd┃
               ┖  ┚
</code></pre>
<p>(Incidentally, the version of Method K given in Wikipedia interchanges
<em>k</em>₂ and <em>k</em>₃, as well as <em>a</em> and <em>c</em>, and <em>b</em> and <em>d</em>.  That’s
because I reconstructed it from memory and it has an asymmetry with
respect to its arguments that Karatsuba multiplication does not.)</p>
<h2>Karatsuba multiplication</h2>
<p>A very similar insight underlies Karatsuba multiplication for
multiple-precision real numbers; I don’t know if Karatsuba was working
directly from the above complex algorithm, but in Karatsuba
multiplication we obtain (<em>a</em> + <em>br</em>)(<em>c</em> + <em>dr</em>) = <em>ac</em> + (<em>ad</em> +
<em>bc</em>)<em>r</em> + <em>bdr</em>² by calculating <em>j</em>₀ + (<em>j</em>₂ - <em>j</em>₁ - <em>j</em>₀)<em>r</em> +
<em>j</em>₁<em>r</em>², where <em>j</em>₀ is of course <em>ac</em> and <em>j</em>₁ is of course <em>bd</em>,
from which we can calculate that <em>j</em>₂ must be <em>ac</em> + <em>ad</em> + <em>bc</em> +
<em>bd</em> = (<em>a</em> + <em>b</em>)(<em>c</em> + <em>d</em>) — our third multiply!  (And two adds.)</p>
<p>So, for example, with <em>r</em> = 10, we can calculate 97 × 86 as <em>j</em>₀ = 7×6
= 42, <em>j</em>₁ = 9×8 = 72, <em>j</em>₂ = (9+7)×(8+6) - <em>j</em>₁ - <em>j</em>₀ = 16×14 - 42 -
72 = 224 - 42 - 72 = 110, so our answer is 42 + 110×10 + 72×100 =
8342, which is correct.  And recursively subdividing the problem this
way gives us Karatsuba’s asymptotically faster multiplication
algorithm, the first one discovered in millennia.</p>
<p>Applied to the special case <em>r</em> = <em>i</em>, Karatsuba multiplication gives
us <em>j</em>₀ - <em>j</em>₁ + (<em>j</em>₂ - <em>j</em>₁ - <em>j</em>₀)<em>i</em>, so Karatsuba multiplication
gives us a slightly different three-real-multiply algorithm for
complex multiplication.  It uses two adds and three subtracts in
addition to the multiplies, while Method K uses three adds and two
subtracts — virtually the same computational cost.</p>
<h2>Partial evaluation</h2>
<p>If we partially evaluate these two algorithms with respect to one of
the arguments, say we hold constant the (<em>a</em>, <em>b</em>) argument, Method K
eliminates an add and a subtract, because <em>k</em>₂ = <em>m</em>₀<em>d</em> and <em>k</em>₃ =
<em>m</em>₁<em>c</em>, where the <em>mᵢ</em> depend only on the constant argument.
Partially evaluating Karatsuba multiplication in the same way only
eliminates the <em>a</em> + <em>b</em> add.  So for complex multiplication by a
constant multiplier, Karatsuba costs three multiplies, one add, and
three subtracts; Method K costs three multiplies, two adds, and one
subtract; and the basic method costs four multiplies, one add, and one
subtract, just as for non-constant arguments.</p>
<h2>Complex multiplication for rotation and scaling</h2>
<p>In the context of computer graphics, complex multiplies are
particularly interesting because they perform uniform scaling and
rotation in a single operation.  So, for example, you can take a
vector figure represented as a list of (<em>x</em>, <em>y</em>) points, and scale it
by <em>n</em> and rotate it by <em>θ</em> by multiplying each number (<em>x</em> + <em>yi</em>) by
a complex constant (<em>n</em> cos <em>θ</em> + <em>ni</em> sin <em>θ</em>).  (Typically you also
want translation, which is complex addition if you’re still thinking
in complex numbers, but there’s no advantage in doing so.)  If you
have the <em>x</em> and <em>y</em> components of various different points in some
SIMD registers, this costs you three SIMD multiplies, two SIMD adds,
and one SIMD subtract using the partially-evaluated Method K.</p>
<p>Commonly in computer graphics we instead rotate raster images; this
can be done by brute force by translating and rotating the screen
coordinates of every screen pixel into texture space by the methods
above, but strength-reducing this operation is very advantageous and
universally done.  Instead of transforming the coordinates of each
pixel, we transform the coordinates of a start pixel, the delta (1, 0)
to move one pixel to the right, and the delta (0, 1) to move one scan
line down.  These give us increments we can use to walk around texture
space with simple adds.  Then we can sample from texture space with a
variety of methods — for procedural textures like fragment shaders, we
just invoke the procedure with the transformed (x, y) arguments, while
for materialized textures we commonly use nearest-neighbor or bilinear
sampling, though there are a variety of common tradeoffs between
aliasing and computation time.</p>
<h2>Paeth’s three-shear rotation algorithm</h2>
<p>There’s another famous algorithm for rotating raster images with three
multiplies, though, by Paeth, usually called the “three-shear
rotation”.  It doesn’t do any scaling, but its compensating virtue is
that, in its usual form, it doesn’t lose any pixels — under
appropriate circumstances it’s perfectly reversible, because you
execute it by <em>shearing</em> the raster pixels by displacing them some
integer number of pixels.  This also means that it doesn’t require
per-pixel sampling operations, even rounding.</p>
<p>The disadvantage of Paeth’s three-shear rotation is that it produces a
lot of aliasing artifacts because of the constraint of shifting the
pixels only by integer amounts.  See <a href="stochastic-fractional-delay-lines.html">the note on stochastic
fractional delay lines</a> for some
approaches to this problem.</p>
<p>Paeth’s algorithm for rotating a vector (<em>x</em>, <em>y</em>) consist of the
following three steps:</p>
<pre><code>x += αy;
y -= βx;
x += αy;
</code></pre>
<p>(And the shear transformations work by shifting pixels <em>αy</em> pixels to
the right in the first step, <em>βx</em> pixels up in the second step, and
<em>αy</em> pixels to the right again in the third step.  Normally you round
these shifts to integers.)</p>
<p>We can represent this calculation with matrix concatenation as follows
(here I’m copying my memory of Tobin Fricke’s page on the subject):</p>
<pre><code>┎       ┒┎       ┒┎       ┒┎   ┒
┃  1  α ┃┃  1  0 ┃┃  1  α ┃┃ x ┃
┃  0  1 ┃┃ -β  1 ┃┃  0  1 ┃┃ y ┃
┖       ┚┖       ┚┖       ┚┖   ┚
</code></pre>
<p>Let’s concatenate out the matrices; first the rightmost ones:</p>
<pre><code>┎       ┒┎       ┒   ┎          ┒
┃  1  0 ┃┃  1  α ┃ = ┃  1   α   ┃
┃ -β  1 ┃┃  0  1 ┃   ┃ -β  1-αβ ┃
┖       ┚┖       ┚   ┖          ┚
</code></pre>
<p>That is, when we subtract off <em>β</em> of <em>x</em> from <em>y</em> in the second step,
the <em>x</em> we’re subtracting already has an <em>α</em> of the original <em>y</em> in
it, so the <em>y</em>-to-<em>y</em> item isn’t 1.  Now the third step:</p>
<pre><code>┎       ┒┎          ┒   ┎                   ┒
┃  1  α ┃┃  1   α   ┃ = ┃ 1 - αβ   2α - α²β ┃
┃  0  1 ┃┃ -β  1-αβ ┃   ┃   -β      1 - αβ  ┃
┖       ┚┖          ┚   ┖                   ┚
</code></pre>
<p>So now we end up subtracting off a proportion <em>αβ</em> of the original <em>x</em>
as well as the original <em>y</em>, and adjusting each one by different
fractions (respectively -<em>β</em> and 2<em>α</em> - <em>α</em>²<em>β</em>) of the other,
fractions which approach 0 if <em>α</em> and <em>β</em> do.</p>
<p>So, to get a matrix for rotation by <em>θ</em> out of this, we need cos <em>θ</em> =
1 - <em>αβ</em> and sin <em>θ</em> = -<em>β</em> = <em>α</em>²<em>β</em> - 2<em>α</em>, which is three equations
in two unknowns.  We have directly that <em>β</em> = -sin <em>θ</em>, and to
calculate <em>α</em> we can observe that cos <em>θ</em> = √(1 - sin² <em>θ</em>) = √(1 -
<em>β</em>²) = 1 - <em>αβ</em>.  Thus <em>α</em> = (1 - √(1 - <em>β</em>²))/<em>β</em>.  (As it turns
out, this is -tan ½<em>θ</em>, but that’s a terrible way to calculate it.)</p>
<p>But our problem was overdetermined, so we still have to check if this
gives us -<em>β</em> = <em>α</em>²<em>β</em> - 2<em>α</em>, so we want to see if <em>β</em> = 2<em>α</em> -
<em>α</em>²<em>β</em>, which becomes<br />
2(1 - √(1 - <em>β</em>²))/<em>β</em> - <em>β</em>(1 - √(1 - <em>β</em>²))²/<em>β</em>² =<br />
2/<em>β</em> - 2√(1 - <em>β</em>²)/<em>β</em> - (1 - √(1 - <em>β</em>²))²/<em>β</em> =<br />
(2 - 2√(1 - <em>β</em>²) - (1 - √(1 - <em>β</em>²))²)/<em>β</em> =<br />
(2 - 2√(1 - <em>β</em>²) - 1 + 2√(1 - <em>β</em>²) - (1 - <em>β</em>²))/<em>β</em> =<br />
(2 - 1 - 1 + <em>β</em>²))/<em>β</em> =<br />
<em>β</em>.</p>
<p>So choosing <em>α</em> and <em>β</em> in this way does give us a pure rotation.  For
example, for <em>θ</em> = 10°, <em>β</em> = -sin <em>θ</em> ≈ -.174, <em>α</em> = (1 - √(1 -
<em>β</em>²))/<em>β</em> ≈ -.0877.  Starting at (100, 0), we proceed to (98.5,
17.4), (93.9, 34.3), (86.5, 50.1).  These all have magnitude 100 and
angles of respectively 0°, 10°, 20°, and 30°, so it seems to be
working, though with a bit of rounding error — the last one should
have been (86.6 = 100√¾, 50.0 = 100/2).</p>
<p><a href="https://www.sciencedirect.com/science/article/abs/pii/S1077316997904202">Toffoli and Quick</a> in 1997 reported a similar three-shear algorithm
for three-dimensional rotation, but I haven’t read the paper.</p>
<h2>Minsky’s circle algorithm and two-shear image rotation</h2>
<p>Paeth’s algorithm is strikingly similar to the Minsky circle algorithm
described in HAKMEM, which computes an approximate rotation of a point
around the origin as follows:</p>
<pre><code>x += αy;
y -= αx;
# no further steps
</code></pre>
<p>This is, surprisingly, stable with exact math; the determinant of the
resulting matrix is exactly 1:</p>
<pre><code>┎          ┒
┃  1   α   ┃
┃ -α  1-α² ┃
┖          ┚
</code></pre>
<p>It’s usually even stable with approximate math, including integer
math.  With integer math, even if <em>α</em> is prescaled to be something
like 3/32, each of the steps is computationally reversible, like
Paeth’s rotations; so orbits can’t converge, and they can’t grow
without bound because the determinant is 1, so they must return to the
starting point.  (For it to be computationally irreversible, you’d
need addition and subtraction to round sometimes.)  But the circles
described by successive iterations are elliptical, which is obvious if
you start with, for example, (<em>x</em>, <em>y</em>, <em>α</em>) = (1, 1, 1) — the orbit
is (1, 1), (2, -1), (2, -1), (1, -2), (-1, -1), (-2, 1), (-1, 2), and
then repeats.  For smaller values of <em>α</em> the ellipticity is quite
small.</p>
<p>The reversibility property means that if you use this transformation
to map a bunch of unique pixel coordinates, the resulting pixel
coordinates will still be unique!  In fact we can implement this
“rotation” on a raster image with two Paeth-like shears, and each of
the pixels will describe a Minsky pseudocircle that never collides
with any other pixel, and eventually returns to its starting point.
The image will be distorted as it rotates (in particular any pixels
close enough to the origin that <em>αx</em> and <em>αy</em> round to zero will stay
put instead of rotating!) but it will eventually return to its
original form.  Not having tried it, I suspect that for many parameter
values, particularly with the center of rotation well outside the
image, the distortions will be imperceptibly small compared to the
aliasing of Paeth’s algorithm implemented with integer shears.</p>
<p>Both Minsky’s and Paeth’s algorithm can be thought of as two timesteps
of leapfrog integration of a simple harmonic oscillator (<em>ẍ</em> = -<em>kx</em>);
the difference is that Paeth’s algorithm starts halfway in between two
timesteps.  But it seems like this would imply that you can undo the
ellipticity of Minsky’s algorithm by picking a different starting
position, and in fact you can’t.</p>
<h2>Paeth locality and run-length slicing</h2>
<p>Many people have questioned whether Paeth rotation is still fast on
modern machines, quite apart from the aliasing artifacts induced by
its standard implementation with integer pixel shifts, because the
standard approach uses three passes over the image, thus blowing up
your dcache unnecessarily.  I think you can probably get enough
locality by pipelining and maybe tiling.  Suppose you break the image
into tiles of 16×16 pixels (768 bytes in fancy shmancy 24-bit color)
and your α and β factors are small enough that the shift over the
whole image is less than ±16 pixels.  Then producing a single output
tile involves only pixels from two horizontally adjacent second-stage
tiles; each of these involves only pixels from two vertically adjacent
first-stage tiles, four in all; and each of these involves only pixels
from two horizontally adjacent input tiles, six in all, spread across
two rows of tiles.</p>
<p>So if you pipeline the shears you only need enough dcache to hold 32
scan lines of the image, which I think is even true without tiling; if
it’s the traditional 1024 pixels wide then that’s 96 kilobytes in
24-bit color, which is coincidentally exactly the size of my L1D cache
on my Pentium N3700 laptop.</p>
<p>We can do better than this with Z-curve or Hilbert-curve ordering over
the tiles.</p>
<p>However, for smallish rotations, we can do <em>much</em> better; each scan
line of the output is made out of concatenated segments from different
scan lines of the input.</p>
<p>Consider an input image all white with one horizontal black line.  The
initial x-shear just moves the black line with the image.  Then the
y-shear breaks the black line up into something like a Bresenham line;
if the y-shear is 16 pixels over a width of 500, for example, it
consists of alternating 31-pixel (75%) and 32-pixel (25%) segments,
placed on successive scan lines; this stretches the
originally-500-pixel line into a line of roughly 500.26 pixels in
length.  Then the final x-shear may overlap some of these segments,
putting two black pixels above one another, or it may not, depending
on where the line is positioned vertically.</p>
<p>I think these overlaps are points where, when moving from writing into
output scan line <em>m</em> by copying from input scan line <em>n</em> to copying
from input scan line <em>n</em>±1, we also adjust the <em>x</em>-offset from which
we are copying.</p>
<p>Larger rotations produce shorter segments which overlap more often,
but until you get to a rotation of more than about 20°, the segments
are still substantial.  You can copy these segments directly from the
input image into their place in the output image, rather like
run-length-slice line drawing, but sometimes with overlap between the
ends of the slices, and copying pixels rather than filling colors.</p><script src="../liabilities/addtoc.js"></script><div><h2>Topics</h2><ul><li><a href="../topics/performance.html">Performance</a> (25 notes)
</li><li><a href="../topics/math.html">Math</a> (13 notes)
</li><li><a href="../topics/graphics.html">Graphics</a> (10 notes)
</li><li><a href="../topics/paeth-rotation.html">Paeth rotation</a> (2 notes)
</li><li><a href="../topics/minsky-algorithm.html">Minsky algorithm</a> (2 notes)
</li><li>Karatsuba</li></ul></div></html>